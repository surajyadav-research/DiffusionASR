{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b31231b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5992fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = transformers.AutoModelForCausalLM.from_pretrained(\"./pretrained-llms/LLaDA-8B-Instruct/\", trust_remote_code=True, device_map=\"auto\")\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"./pretrained-llms/LLaDA-8B-Instruct/\", trust_remote_code=True)\n",
    "tokenizer.eot_id = 126348"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d86147a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "def get_num_transfer_tokens(mask_index, steps):\n",
    "    mask_num = mask_index.sum(dim=1, keepdim=True)\n",
    "    base = mask_num // steps\n",
    "    remainder = mask_num % steps\n",
    "    num_transfer_tokens = torch.zeros(mask_num.size(0), steps, device=mask_index.device, dtype=torch.int64) + base\n",
    "    for i in range(mask_num.size(0)):\n",
    "        num_transfer_tokens[i, :remainder[i]] += 1\n",
    "    return num_transfer_tokens\n",
    "\n",
    "def forward_process(input_ids, eps=1e-3):\n",
    "    b, l = input_ids.shape\n",
    "    t = torch.rand(b, device=input_ids.device)\n",
    "    p_mask = (1 - eps) * t + eps\n",
    "    p_mask = p_mask[:, None].repeat(1, l)\n",
    "    masked_indices = torch.rand((b, l), device=input_ids.device) < p_mask\n",
    "    noisy_batch = torch.where(masked_indices, 126336, input_ids)\n",
    "    return noisy_batch, masked_indices, p_mask\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate(model, input_ids, steps=128, gen_length=128, mask_id=126336):\n",
    "    B, L = input_ids.shape\n",
    "    x = torch.full((B, L + gen_length), mask_id, dtype=torch.long, device=model.device)\n",
    "    x[:, :L] = input_ids.clone()\n",
    "    \n",
    "    mask_index = (x == mask_id)\n",
    "    num_transfer_tokens = get_num_transfer_tokens(mask_index, steps)\n",
    "    \n",
    "    for i in range(steps):\n",
    "        mask_index = (x == mask_id)\n",
    "        attention_mask = (~mask_index).long()\n",
    "        \n",
    "        logits = model(x, attention_mask=attention_mask).logits\n",
    "        x0 = torch.argmax(logits, dim=-1)\n",
    "        \n",
    "        x0_p = torch.rand(x0.shape, device=x0.device)\n",
    "        x0 = torch.where(mask_index, x0, x)\n",
    "        confidence = torch.where(mask_index, x0_p, -np.inf)\n",
    "        \n",
    "        transfer_index = torch.zeros_like(x0, dtype=torch.bool, device=x0.device)\n",
    "        for j in range(B):\n",
    "            _, select_index = torch.topk(confidence[j], k=num_transfer_tokens[j, i])\n",
    "            transfer_index[j, select_index] = True\n",
    "        x[transfer_index] = x0[transfer_index]\n",
    "    \n",
    "    return x\n",
    "\n",
    "sample_data = [\n",
    "    {\"prompt\": \"What is 2+2?\", \"target\": \"4\"},\n",
    "    {\"prompt\": \"What color is the sky?\", \"target\": \"Blue\"},\n",
    "    {\"prompt\": \"Capital of France?\", \"target\": \"Paris\"},\n",
    "    {\"prompt\": \"How many days in a week?\", \"target\": \"Seven\"},\n",
    "    {\"prompt\": \"What is H2O?\", \"target\": \"Water\"},\n",
    "]\n",
    "\n",
    "# tokenize using chat template\n",
    "def prepare_sample(sample, tokenizer):\n",
    "    messages = [{\"role\": \"user\", \"content\": sample[\"prompt\"]}]\n",
    "    prompt_ids = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True)\n",
    "    target_ids = tokenizer.encode(sample[\"target\"], add_special_tokens=False) + [tokenizer.eot_id] + [tokenizer.eos_token_id]\n",
    "    full_ids = prompt_ids + target_ids\n",
    "    return torch.tensor(full_ids), len(prompt_ids)\n",
    "\n",
    "tokenized_samples = [prepare_sample(s, tokenizer) for s in sample_data]\n",
    "\n",
    "# pad the batch\n",
    "max_len = max(ids.shape[0] for ids, _ in tokenized_samples)\n",
    "padded_input_ids = torch.zeros(len(tokenized_samples), max_len, dtype=torch.long)\n",
    "prompt_lengths = torch.zeros(len(tokenized_samples), dtype=torch.long)\n",
    "for i, (ids, prompt_len) in enumerate(tokenized_samples):\n",
    "    padded_input_ids[i, :ids.shape[0]] = ids\n",
    "    padded_input_ids[i, ids.shape[0]:] = tokenizer.pad_token_id if tokenizer.pad_token_id else 0\n",
    "    prompt_lengths[i] = prompt_len\n",
    "\n",
    "device = next(model.parameters()).device\n",
    "padded_input_ids = padded_input_ids.to(device)\n",
    "prompt_lengths = prompt_lengths.to(device)\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "embed_dim = model.model.transformer.wte.weight.shape[1]\n",
    "\n",
    "linear_adapter = torch.nn.Sequential(\n",
    "    torch.nn.LayerNorm(embed_dim),\n",
    "    torch.nn.Linear(embed_dim, 64),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(64, embed_dim),\n",
    "    torch.nn.LayerNorm(embed_dim),\n",
    ").to(device)\n",
    "\n",
    "def train_step_with_adapter(model, linear_adapter, input_ids, prompt_lengths):\n",
    "    noisy_batch, _, p_mask = forward_process(input_ids)\n",
    "    \n",
    "    # Shape: (batch_size, seq_len) - each row contains [0, 1, 2, ..., seq_len-1]\n",
    "    token_positions = torch.arange(noisy_batch.shape[1], device=noisy_batch.device).expand(noisy_batch.size(0), noisy_batch.size(1))\n",
    "    prompt_mask = (token_positions < prompt_lengths.unsqueeze(1))\n",
    "    noisy_batch[prompt_mask] = input_ids[prompt_mask]\n",
    "    \n",
    "    answer_lengths = torch.sum((1 - prompt_mask.to(torch.int64)), dim=-1, keepdim=True)\n",
    "    # answer_lengths shape before: (batch_size, 1) - e.g., tensor([[15], [12], [14]])\n",
    "    # After repeat(1, noisy_batch.shape[1]): (batch_size, seq_len)\n",
    "    # Each row contains the same value repeated seq_len times\n",
    "    # e.g., tensor([[15, 15, 15, ..., 15],\n",
    "    #               [12, 12, 12, ..., 12],\n",
    "    #               [14, 14, 14, ..., 14]])\n",
    "    # Values represent the number of answer tokens (non-prompt tokens) for each sample\n",
    "    # NOTE: This includes padding tokens! If sequences are padded, answer_lengths will\n",
    "    # count padding as part of the \"answer\" region (everything after the prompt).\n",
    "    # This may cause issues if padding tokens are masked and included in the loss.\n",
    "    answer_lengths = answer_lengths.repeat(1, noisy_batch.shape[1])\n",
    "    \n",
    "    # NOTE: This includes padded tokens that were masked! If a padding token was\n",
    "    # randomly masked during forward_process, it will be included here and contribute to the loss.\n",
    "    masked_indices = (noisy_batch == 126336)\n",
    "    \n",
    "    embeddings = model.model.transformer.wte(noisy_batch)\n",
    "    adapted_embeddings = embeddings + linear_adapter(embeddings)\n",
    "    logits = model(inputs_embeds=adapted_embeddings).logits\n",
    "    \n",
    "    # NOTE: The loss is calculated on ALL masked tokens, including padded tokens that were masked.\n",
    "    # This means if padding tokens (pad_token_id) were randomly masked during forward_process,\n",
    "    # the model will learn to predict them. This implicitly teaches the model about sequence length\n",
    "    # and when to \"end\" the answer. If you want to exclude padding from the loss, you would need\n",
    "    # to create a padding_mask and combine it with masked_indices:\n",
    "    # e.g., valid_mask = masked_indices & (input_ids != tokenizer.pad_token_id)\n",
    "    token_loss = F.cross_entropy(logits[masked_indices], input_ids[masked_indices], reduction='none') / p_mask[masked_indices]\n",
    "    loss = torch.sum(token_loss / answer_lengths[masked_indices]) / input_ids.shape[0]\n",
    "    \n",
    "    return loss\n",
    "\n",
    "num_epochs = 200\n",
    "optimizer = torch.optim.AdamW(linear_adapter.parameters(), lr=1e-3)\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "linear_adapter.train()\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    train_loss = train_step_with_adapter(model, linear_adapter, padded_input_ids, prompt_lengths)\n",
    "    train_loss.backward()\n",
    "    optimizer.step()\n",
    "    train_losses.append(train_loss.item())\n",
    "    \n",
    "    linear_adapter.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = train_step_with_adapter(model, linear_adapter, padded_input_ids, prompt_lengths)\n",
    "        val_losses.append(val_loss.item())\n",
    "    linear_adapter.train()\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {train_loss.item():.4f}, Val Loss: {val_loss.item():.4f}\")\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Val Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c508e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Generation Demo ===\n",
    "sample_text = \"are you a chat LLM?\"\n",
    "sample_text_ids = tokenizer.apply_chat_template([{\"role\": \"user\", \"content\": sample_text}], tokenize=True, add_generation_prompt=True)\n",
    "sample_text_ids = torch.tensor(sample_text_ids).unsqueeze(0)\n",
    "\n",
    "output = generate(\n",
    "    model=model,\n",
    "    input_ids=sample_text_ids,\n",
    ")\n",
    "print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88c6bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "linear_adapter.eval()\n",
    "with torch.no_grad():\n",
    "    for sample in sample_data:\n",
    "        messages = [{\"role\": \"user\", \"content\": sample[\"prompt\"]}]\n",
    "        prompt_ids = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True)\n",
    "        input_ids = torch.tensor(prompt_ids).unsqueeze(0).to(device)\n",
    "        \n",
    "        embeddings = model.model.transformer.wte(input_ids)\n",
    "        adapted_embeddings = embeddings + linear_adapter(embeddings)\n",
    "        \n",
    "        output_without_adapter = generate(model=model, input_ids=input_ids)\n",
    "        \n",
    "        gen_length = 8\n",
    "        mask_id = 126336\n",
    "        \n",
    "        x = torch.full((1, input_ids.shape[1] + gen_length), mask_id, dtype=torch.long, device=device)\n",
    "        x[:, :input_ids.shape[1]] = input_ids\n",
    "        \n",
    "        mask_token_embedding = model.model.transformer.wte(torch.tensor([mask_id], device=device))\n",
    "        sequence_embeds = torch.cat([adapted_embeddings, mask_token_embedding.expand(1, gen_length, -1)], dim=1)\n",
    "        \n",
    "        steps = 32\n",
    "        for step in range(steps):\n",
    "            mask_index = (x == mask_id)\n",
    "            logits = model(inputs_embeds=sequence_embeds).logits\n",
    "            x0 = torch.argmax(logits, dim=-1)\n",
    "            \n",
    "            p = F.softmax(logits, dim=-1)\n",
    "            x0_p = torch.gather(p, dim=-1, index=x0.unsqueeze(-1)).squeeze(-1)\n",
    "            \n",
    "            x0 = torch.where(mask_index, x0, x)\n",
    "            confidence = torch.where(mask_index, x0_p, -float('inf'))\n",
    "            \n",
    "            num_transfer = max(1, mask_index.sum().item() // (steps - step))\n",
    "            _, select_index = torch.topk(confidence.flatten(), k=min(num_transfer, mask_index.sum().item()))\n",
    "            transfer_mask = torch.zeros_like(x, dtype=torch.bool)\n",
    "            transfer_mask.view(-1)[select_index] = True\n",
    "            x[transfer_mask] = x0[transfer_mask]\n",
    "            \n",
    "            if transfer_mask.any():\n",
    "                new_embeds = model.model.transformer.wte(x0[transfer_mask])\n",
    "                sequence_embeds[transfer_mask] = new_embeds\n",
    "        \n",
    "        output_with_adapter = x\n",
    "        \n",
    "        print(f\"Prompt: {sample['prompt']}\")\n",
    "        print(f\"Expected: {sample['target']}\")\n",
    "        print(f\"Without adapter: {tokenizer.decode(output_without_adapter[0], skip_special_tokens=False)}\")\n",
    "        print(f\"With adapter: {tokenizer.decode(output_with_adapter[0], skip_special_tokens=False)}\")\n",
    "        print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207f4518",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from joblib import load\n",
    "from src.llm import LLaDA_8B_Instruct\n",
    "from src.dataset import AsrDataset\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c8a3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LLaDA_8B_Instruct()\n",
    "dataset = AsrDataset(\n",
    "    tokenizer=model.tokenizer,\n",
    "    prompt=\"Transcribe speech to text. \",\n",
    "    mel_size=128,\n",
    "    fix_length_audio=-1,\n",
    "    fix_audio_duration=30,\n",
    "    inference_mode=False,\n",
    "    normalize=True,\n",
    "    input_type=\"mel\",\n",
    "    path_to_jsonl_file=\"/mnt/disk-n8/data/librispeech/manifests/test-other.jsonl\",\n",
    "    target_column=\"processed_target\",\n",
    "    llm_name=\"llada-8b-instruct\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc44a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.generate(\n",
    "    input_ids=batch[\"input_ids\"],\n",
    "    attention_mask=batch[\"attention_mask\"],\n",
    "    max_new_tokens=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc1372b",
   "metadata": {},
   "outputs": [],
   "source": [
    "thingy = dataset[5]\n",
    "thingy[\"input_ids\"][thingy[\"input_ids\"] == -1] = 0\n",
    "print(thingy[\"input_ids\"])\n",
    "model.tokenizer.decode(thingy[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd5417f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=3, collate_fn=dataset.collator)\n",
    "for batch in dataloader:\n",
    "    break\n",
    "\n",
    "input_ids = batch[\"input_ids\"].clone()\n",
    "input_ids[input_ids == -1] = 126081\n",
    "for i, input_ids in enumerate(model.tokenizer.batch_decode(input_ids)):\n",
    "    print(\"----\")\n",
    "    \n",
    "    print(input_ids)\n",
    "    print(batch[\"input_ids\"][i])\n",
    "    print(batch[\"audio_mel\"][i].shape)\n",
    "    plt.imshow(batch[\"audio_mel\"][i])\n",
    "    plt.show()\n",
    "    plt.plot(batch[\"attention_mask\"][i])\n",
    "    plt.title(\"Attention Mask\")\n",
    "    plt.show()\n",
    "    # print(batch[\"audio_mask\"][i])\n",
    "    # print(batch[\"audio_raw\"][i].shape)\n",
    "    # print(batch[\"audio_raw_mask\"][i])\n",
    "    # print(batch[\"audio_mel\"][i].shape)\n",
    "    print(batch[\"modality_mask\"][i])\n",
    "    print(batch[\"audio_duration\"][i])\n",
    "    print(batch[\"labels\"][i])\n",
    "    print(\"-----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353024cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.tokenizer.decode([      6,     ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781f3f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = load(\"batch_0.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e58c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[\"input_ids\"][0][batch[\"input_ids\"][0] == -1] = 126081"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b5877f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"transcribe the following audio\"\n",
    "label = \"this is what the audio says\"\n",
    "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "prompt_ids = model.tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True)\n",
    "\n",
    "num_audio_tokens = 10\n",
    "audio_placeholder_ids = [-1] * num_audio_tokens\n",
    "\n",
    "# Find where assistant section starts (after <eot_id><start_id>assistant<end_id>\\n)\n",
    "# Insert audio placeholders right before the assistant's response\n",
    "eot_id = model.tokenizer.encode(\"<|eot_id|>\", add_special_tokens=False)[0]\n",
    "eot_idx = prompt_ids.index(eot_id)\n",
    "\n",
    "# Split: user message part + audio placeholders + assistant part\n",
    "user_part = prompt_ids[:eot_idx]  # up to but not including <eot_id>\n",
    "assistant_header = prompt_ids[eot_idx:]  # <eot_id><start_id>assistant<end_id>\\n (eot comes after audio)\n",
    "\n",
    "full_ids = user_part + audio_placeholder_ids + assistant_header\n",
    "print(\"Token IDs:\", full_ids)\n",
    "print(\"Audio placeholder starts at index:\", len(user_part))\n",
    "\n",
    "decoded_text = model.tokenizer.decode([t if t >= 0 else 0 for t in full_ids])\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4598b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016f64ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089fea4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.tokenizer.decode(model.tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c02df4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.tokenizer.decode(model.tokenizer([\"<BOS><start_id>user<end_id>\\n{prompt}<eot_id><start_id>assistant<end_id>\\n{target}<EOS>\"])[\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eee44bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in model.tokenizer.batch_decode(batch[\"input_ids\"][0]):\n",
    "    print(token, end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f32169",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump, load\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "input_ids = load(\"input_embeds_attention_mask_labels.joblib\")[0]\n",
    "attention_masks = load(\"input_embeds_attention_mask_labels.joblib\")[1]\n",
    "labels = load(\"input_embeds_attention_mask_labels.joblib\")[2]\n",
    "for i, thing in enumerate(input_ids):\n",
    "    print(thing.shape)\n",
    "    plt.imshow(thing.cpu().detach().numpy())\n",
    "    plt.title(f\"Input ID {i}\")\n",
    "    plt.show()\n",
    "    plt.plot(attention_masks[i].cpu().detach().numpy())\n",
    "    plt.title(f\"Attention Mask {i}\")\n",
    "    plt.show()\n",
    "    print(labels[i].cpu().detach().numpy())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "suksham",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
